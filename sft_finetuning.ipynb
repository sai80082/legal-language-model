{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Instruction', 'Response'],\n",
       "    num_rows: 25607\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"vishnun0027/Indian-Law\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction\n",
      "What is the difference between a petition and a plaint in Indian law?\n"
     ]
    }
   ],
   "source": [
    "print(\"Instruction\")\n",
    "print(dataset[0][\"Instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response\n",
      "A petition is a formal request submitted to a courttribunalor authority to seek a specific remedy or relief. It is commonly used for various purposessuch as filing a writ petition in the High Court or submitting a petition for divorce. On the other handa plaint is a formal written statement of a plaintiff's claim in a civil lawsuit. The key difference is that a petition is more versatile and can be used for various legal matterswhile a plaint is specific to civil cases.\n"
     ]
    }
   ],
   "source": [
    "print(\"Response\")\n",
    "print(dataset[0][\"Response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Instruction', 'Response'],\n",
       "    num_rows: 25600\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"Instruction\"] is not None and x[\"Response\"] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Instruction', 'Response'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern = r\"[###]\" # identify the rows which have special characters except essentials\n",
    "filtered_df = dataset.filter(lambda x: re.match(pattern,x[\"Instruction\"]) or re.match(pattern,x[\"Response\"]) )\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Draft a hypothetical legal petition based on the provided case.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Cleaned Text\n",
      "Draft a hypothetical legal petition based on the provided case.\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df[1][\"Instruction\"])\n",
    "cleaned_text = re.sub(r'^### (Instruction|Response):.*\\n?', '', filtered_df[1][\"Instruction\"], flags=re.MULTILINE)\n",
    "print(\"Cleaned Text\")\n",
    "print(cleaned_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(row):\n",
    "    if re.match(pattern,row[\"Instruction\"]) or re.match(pattern,row[\"Response\"]):\n",
    "        cleaned_text = re.sub(r'^### (Instruction|Response):.*\\n?', '', row[\"Instruction\"], flags=re.MULTILINE)\n",
    "        row[\"Instruction\"] = cleaned_text.strip()\n",
    "    return row\n",
    "dataset = dataset.map(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\",device_map=\"auto\")\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-legal-india\"\n",
    "finetune_tags = [\"smol\", \"leagal-india\",\"indian law\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "Can a Vakalatnama be revoked or withdrawn in India?\n",
      "assistant\n",
      "Yes, a Vakalatnama can be revoked or withdrawn in India. The Indian Constitution provides for revocation of a Vakalatnama under the provisions of Section 12 of the Indian Constitution. However, the process for revoking a Vakalatnama can be complex and may involve a court hearing, a hearing by a special authority, or a court martial.\n",
      "\n",
      "The Indian Constitution also provides for the right to a speedy trial\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can a Vakalatnama be revoked or withdrawn in India?\"\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(example):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example['Instruction']},\n",
    "        {\"role\": \"assistant\", \"content\": example['Response']}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d43ebac50fc4ab1b645f3b037af4f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the chat templatefunction to the dataset\n",
    "chat_df = dataset.map(apply_chat_template)\n",
    "chat_df = chat_df.train_test_split(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(example['prompt'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokens['labels'] = [\n",
    "        -100 if token == tokenizer.pad_token_id else token for token in tokens['input_ids']\n",
    "    ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a5b129ebc6405ca608f7d839bb0565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9667015e5a3443b9b1c5f6e231bcb411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenize_function to each row\n",
    "tokenized_dataset = chat_df.map(tokenize_function)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['Instruction', 'Response', 'prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=16,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=100,  # Frequency of logging training metrics\n",
    "    save_steps=200,  # Frequency of saving model checkpoints\n",
    "    eval_steps=200,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),\n",
    "    hub_model_id=finetune_name,\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.193200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.118400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.086100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efbd7f26a064d80a512c8c9eff691e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8b018fec5f4c13ac76ed0f5fe0bab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2014651bba9443ba8bfe7b8ceaec2768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/saicharan1010/SmolLM2-FT-legal-india/commit/a3ab5513364d700fda05e4dc18659a2a72ffbaae', commit_message='End of training', commit_description='', oid='a3ab5513364d700fda05e4dc18659a2a72ffbaae', pr_url=None, repo_url=RepoUrl('https://huggingface.co/saicharan1010/SmolLM2-FT-legal-india', endpoint='https://huggingface.co', repo_type='model', repo_id='saicharan1010/SmolLM2-FT-legal-india'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "Can a Vakalatnama be revoked or withdrawn in India?\n",
      "assistant\n",
      "Yes, a Vakalatnama can be revoked or withdrawn in India. According to the context provided, \"The revocation of a Vakalatnama shall be in the nature of a law made by the Legislature of the State in which the Vakalatnama is to be found.\" This means that the revocation or withdrawal of a Vakalatnama is a legal process that can be performed by the State Legislature. However, it is important to note that this is not a legal process that can be performed by the Supreme Court or any other authority in India. The context does not specify any specific procedure for revoking or withdrawing a Vakalatnama. Therefore, it is not possible to determine whether a Vakalatnama can be revoked or withdrawn in India based on this context alone.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can a Vakalatnama be revoked or withdrawn in India?\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 01:03:31,104 - INFO - Loading dataset...\n",
      "2025-03-01 01:03:31,105 - INFO - Dataset size: 1280 samples.\n",
      "2025-03-01 01:03:31,193 - INFO - Loading models...\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "2025-03-01 01:03:32,414 - INFO - Models loaded successfully!\n",
      "2025-03-01 01:03:32,415 - INFO - Generating responses for Model 1...\n",
      "2025-03-01 01:03:32,415 - INFO - Generating responses for 1280 samples in batches of 16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87ba4056e92443ea93185926d20ce01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 01:05:48,591 - INFO - Total time: 136.175 sec | Avg response time: 0.106 sec\n",
      "2025-03-01 01:05:48,591 - INFO - Generating responses for Model 2...\n",
      "2025-03-01 01:05:48,591 - INFO - Generating responses for 1280 samples in batches of 16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1a61927c294fee9618f2038835e2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 01:07:55,627 - INFO - Total time: 127.035 sec | Avg response time: 0.099 sec\n",
      "2025-03-01 01:07:55,628 - INFO - Response generation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "dataset = dataset[\"test\"]\n",
    "\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./SmolLM2-FT-legal-india\", padding_side=\"left\", truncation=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set\n",
    "\n",
    "# Update pipeline calls\n",
    "model_1 = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"./SmolLM2-FT-legal-india\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    truncation=True  # Explicitly enable truncation\n",
    ")\n",
    "\n",
    "model_2 = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"HuggingFaceTB/SmolLM2-135M-Instruct\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    truncation=True  # Explicitly enable truncation\n",
    ")\n",
    "\n",
    "\n",
    "# Function to generate responses in batches\n",
    "def generate_responses(model, dataset, input_column=\"Instruction\", batch_size=16, max_length=200):\n",
    "    responses = []\n",
    "    total_time = 0\n",
    "    print(f\"Generating responses for {len(dataset)} samples in batches of {batch_size}...\")\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for output in tqdm(model(KeyDataset(dataset, input_column), batch_size=batch_size, max_length=max_length, do_sample=True), total=len(dataset)):\n",
    "            responses.append(output[0][\"generated_text\"])\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        avg_time = total_time / len(dataset)\n",
    "        print(f\"Total time: {total_time:.3f} sec | Avg response time: {avg_time:.3f} sec\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch processing: {e}\")\n",
    "\n",
    "    return responses\n",
    "\n",
    "# Generate responses efficiently\n",
    "print(\"Generating responses for Model 1...\")\n",
    "responses_1 = generate_responses(model_1, dataset)\n",
    "\n",
    "print(\"Generating responses for Model 2...\")\n",
    "responses_2 = generate_responses(model_2, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (fine tunned): {'BLEU': 0.12600565632766575, 'ROUGE': 0.3042493416159143}\n",
      "Model 2: {'BLEU': 0.1206421911321742, 'ROUGE': 0.34455162376518916}\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "smooth_func = SmoothingFunction().method1  # Smoothing function to avoid zero BLEU scores\n",
    "\n",
    "def evaluate_responses(predictions, dataset, ground_truth_column=\"Response\"):\n",
    "    bleu_scores, rouge_scores = [], []\n",
    "    \n",
    "    for pred, gt in zip(predictions, dataset[ground_truth_column]):\n",
    "        pred_tokens, gt_tokens = pred.split(), gt.split()\n",
    "\n",
    "        # Compute BLEU score with smoothing\n",
    "        if pred_tokens:\n",
    "            bleu_scores.append(sentence_bleu([gt_tokens], pred_tokens, smoothing_function=smooth_func))\n",
    "        else:\n",
    "            bleu_scores.append(0.0)  # If prediction is empty, BLEU score is 0\n",
    "\n",
    "        # Compute ROUGE score\n",
    "        rouge_scores.append(rouge.get_scores(pred, gt)[0])\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "        \"ROUGE\": sum([score[\"rouge-l\"][\"f\"] for score in rouge_scores]) / len(rouge_scores),\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "metrics_1 = evaluate_responses(responses_1, dataset)\n",
    "metrics_2 = evaluate_responses(responses_2, dataset)\n",
    "\n",
    "print(\"Model 1 (fine tunned):\", metrics_1)\n",
    "print(\"Model 2:\", metrics_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
